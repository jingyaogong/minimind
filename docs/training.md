# æ¨¡å‹è®­ç»ƒ

æœ¬é¡µé¢ä»‹ç»å¦‚ä½•ä» 0 å¼€å§‹è®­ç»ƒ MiniMind è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“Š æ•°æ®å‡†å¤‡

### 1. ä¸‹è½½æ•°æ®é›†

ä» [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) æˆ– [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset) ä¸‹è½½æ•°æ®é›†ã€‚

åˆ›å»º `./dataset` ç›®å½•å¹¶æ”¾å…¥æ•°æ®æ–‡ä»¶ï¼š

```bash
./dataset/
â”œâ”€â”€ pretrain_hq.jsonl (1.6GB, âœ¨æ¨è)
â”œâ”€â”€ sft_mini_512.jsonl (1.2GB, âœ¨æ¨è)
â”œâ”€â”€ sft_512.jsonl (7.5GB)
â”œâ”€â”€ sft_1024.jsonl (5.6GB)
â”œâ”€â”€ sft_2048.jsonl (9GB)
â”œâ”€â”€ dpo.jsonl (909MB)
â”œâ”€â”€ r1_mix_1024.jsonl (340MB)
â””â”€â”€ lora_*.jsonl
```

!!! tip "æ¨èç»„åˆ"
    æœ€å¿«é€Ÿåº¦å¤ç°ï¼š`pretrain_hq.jsonl` + `sft_mini_512.jsonl`
    
    **å•å¡ 3090 ä»…éœ€ 2 å°æ—¶ + 3 å—é’±ï¼**

### 2. æ•°æ®æ ¼å¼

**é¢„è®­ç»ƒæ•°æ®** (`pretrain_hq.jsonl`):
```json
{"text": "å¦‚ä½•æ‰èƒ½æ‘†è„±æ‹–å»¶ç—‡ï¼Ÿæ²»æ„ˆæ‹–å»¶ç—‡å¹¶ä¸å®¹æ˜“..."}
```

**SFT æ•°æ®** (`sft_*.jsonl`):
```json
{
  "conversations": [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼"}
  ]
}
```

## ğŸ¯ è®­ç»ƒæµç¨‹

æ‰€æœ‰è®­ç»ƒè„šæœ¬ä½äº `./trainer` ç›®å½•ã€‚

### 1. é¢„è®­ç»ƒï¼ˆPretrainï¼‰

é¢„è®­ç»ƒé˜¶æ®µè®©æ¨¡å‹å­¦ä¹ åŸºç¡€çŸ¥è¯†ï¼Œç›®æ ‡æ˜¯**å­¦ä¼šè¯è¯­æ¥é¾™**ã€‚

```bash
cd trainer
python train_pretrain.py

# å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node 2 train_pretrain.py
```

è¾“å‡ºæƒé‡ï¼š`./out/pretrain_*.pth`

!!! info "è®­ç»ƒæ—¶é•¿"
    - MiniMind2-Small (26M): ~1.1h (å•å¡ 3090)
    - MiniMind2 (104M): ~3.9h (å•å¡ 3090)

### 2. ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰

SFT é˜¶æ®µè®©æ¨¡å‹å­¦ä¹ å¯¹è¯æ–¹å¼ï¼Œé€‚åº”èŠå¤©æ¨¡æ¿ã€‚

```bash
python train_full_sft.py

# å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node 2 train_full_sft.py
```

è¾“å‡ºæƒé‡ï¼š`./out/full_sft_*.pth`

!!! info "è®­ç»ƒæ—¶é•¿"
    - MiniMind2-Small: ~1h (ä½¿ç”¨ sft_mini_512)
    - MiniMind2: ~3.3h (ä½¿ç”¨ sft_mini_512)

### 3. LoRA å¾®è°ƒï¼ˆå¯é€‰ï¼‰

LoRA æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€‚åˆé¢†åŸŸé€‚é…ã€‚

```bash
python train_lora.py
```

**åº”ç”¨åœºæ™¯**ï¼š
- åŒ»ç–—é—®ç­”ï¼šä½¿ç”¨ `lora_medical.jsonl`
- è‡ªæˆ‘è®¤çŸ¥ï¼šä½¿ç”¨ `lora_identity.jsonl`

è¾“å‡ºæƒé‡ï¼š`./out/lora/lora_*.pth`

### 4. DPO å¼ºåŒ–å­¦ä¹ ï¼ˆå¯é€‰ï¼‰

DPO ç”¨äºä¼˜åŒ–æ¨¡å‹å›å¤è´¨é‡ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»åå¥½ã€‚

```bash
python train_dpo.py
```

è¾“å‡ºæƒé‡ï¼š`./out/rlhf_*.pth`

### 5. æ¨ç†æ¨¡å‹è’¸é¦ï¼ˆå¯é€‰ï¼‰

è’¸é¦ DeepSeek-R1 çš„æ¨ç†èƒ½åŠ›ã€‚

```bash
python train_distill_reason.py
```

è¾“å‡ºæƒé‡ï¼š`./out/reason_*.pth`

## ğŸ“ˆ æ¨¡å‹ç»“æ„

MiniMind ä½¿ç”¨ Transformer Decoder-Only ç»“æ„ï¼ˆç±»ä¼¼ Llama3ï¼‰ï¼š

![structure](images/LLM-structure.png)

### æ¨¡å‹å‚æ•°é…ç½®

| Model Name | params | d_model | n_layers | kv_heads | q_heads |
|------------|--------|---------|----------|----------|---------|
| MiniMind2-Small | 26M | 512 | 8 | 2 | 8 |
| MiniMind2-MoE | 145M | 640 | 8 | 2 | 8 |
| MiniMind2 | 104M | 768 | 16 | 2 | 8 |

## ğŸ§ª æµ‹è¯•æ¨¡å‹

```bash
# model_mode: 0=pretrain, 1=sft, 2=rlhf, 3=reason
python eval_model.py --model_mode 1

# æµ‹è¯• LoRA æ¨¡å‹
python eval_model.py --lora_name 'lora_medical' --model_mode 2
```

## ğŸ”§ å¤šå¡è®­ç»ƒ

### DDP æ–¹å¼

```bash
torchrun --nproc_per_node N train_xxx.py
```

### DeepSpeed æ–¹å¼

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```

### Wandb ç›‘æ§

```bash
# éœ€è¦å…ˆç™»å½•
wandb login

# å¯ç”¨ wandb
torchrun --nproc_per_node N train_xxx.py --use_wandb
```

## ğŸ’° è®­ç»ƒæˆæœ¬

åŸºäºå•å¡ NVIDIA 3090ï¼š

| æ•°æ®é›†ç»„åˆ | æ—¶é•¿ | æˆæœ¬ | æ•ˆæœ |
|-----------|------|------|------|
| pretrain_hq + sft_mini_512 | 2.1h | â‰ˆ2.73ï¿¥ | ğŸ˜ŠğŸ˜Š åŸºç¡€å¯¹è¯ |
| å®Œæ•´æ•°æ®é›† (MiniMind2-Small) | 38h | â‰ˆ49.61ï¿¥ | ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š å®Œæ•´èƒ½åŠ› |
| å®Œæ•´æ•°æ®é›† (MiniMind2) | 122h | â‰ˆ158.6ï¿¥ | ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š æœ€å¼ºæ€§èƒ½ |

!!! success "æé€Ÿå¤ç°"
    ä½¿ç”¨ `pretrain_hq` + `sft_mini_512`ï¼Œå•å¡ 3090 ä»…éœ€ **2 å°æ—¶ + 3 å—é’±**å³å¯è®­ç»ƒå‡ºèƒ½å¯¹è¯çš„ ChatBotï¼

## ğŸ“ å¸¸è§é—®é¢˜

- **æ˜¾å­˜ä¸è¶³**ï¼šå‡å° `batch_size` æˆ–ä½¿ç”¨ DeepSpeed
- **è®­ç»ƒä¸æ”¶æ•›**ï¼šè°ƒæ•´å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ•°æ®è´¨é‡
- **å¤šå¡è®­ç»ƒæŠ¥é”™**ï¼šç¡®ä¿æ‰€æœ‰å¡éƒ½å¯è§ä¸” CUDA ç‰ˆæœ¬ä¸€è‡´

